Great — I’ll use the Guidelines you uploaded as the authority for which parts of Russell & Norvig to cover, and then explain the topics from the book for each specified section. I’ve followed the guideline mapping exactly (source: your Guidelines_AI.pdf). 

Below I give a focused, exam-ready thorough summary for each Unit and each specified chapter/section from Artificial Intelligence: A Modern Approach (4e). I keep each explanation concise but complete enough to study from — definitions, key ideas, formulas/pseudocode where essential, and exam-style takeaways. Where I draw material directly from the book I cite the book PDF you uploaded. 


---

Unit I — Introduction (Guideline: Chapter 1: 1.1, 1.4; Chapter 2: Complete). 

Chapter 1 — Selected sections

1.1 What is AI? — core definitions & paradigms (book summary)

AI as study of agents: AI studies agents that receive percepts and take actions to maximize some notion of success (performance measure). Think agent = function(percept_sequence → action). Key agent types: reflex, model-based, goal-based, utility-based, learning agents. 

Four historical approaches: (1) Acting humanly (Turing test); (2) Thinking humanly (cognitive modelling); (3) Thinking rationally (laws of thought / logic); (4) Acting rationally (rational agent). Each gives different methods and evaluation criteria. 

Important exam points: define rational agent, explain difference between rational and human-like, and list required capabilities (NLP, knowledge representation, reasoning, learning, perception, robotics for total Turing). 


1.4 The state of the art — snapshot of capabilities & focus (book)

Modern emphasis: More focus on machine learning (esp. deep learning), probabilistic methods, probabilistic programming, multiagent systems; growth of NLP, vision, robotics using ML. Recognize tradeoffs: data-driven vs knowledge-engineered systems. 

Risks & benefits preview: this section foreshadows ethical/safety topics (useful for Unit V later). Key takeaway: know the major subfields and why ML expanded AI’s reach. 


Chapter 2 — Intelligent Agents (Complete — high-yield points)

2.1 Agents & environments: agent architecture (sensors, actuators), percept sequence, agent function & program. Environment properties: fully/partially observable, deterministic/stochastic, episodic/sequential, static/dynamic, discrete/continuous, single-/multi-agent. Understand examples and how environment properties affect agent design. 

2.2 Rationality: define rationality as doing the right thing given percepts, prior knowledge, and performance measure. Distinguish performance measure (external evaluator) from agent’s internal information. Consider limited computational resources — bounded or limited rationality. 

2.3 Nature of environments: show consequences of observability, determinism, episodicity — e.g., partial observability → need for belief states. Multi-agent → game-theoretic considerations. 

2.4 Structure of agents: agent program architectures: simple reflex, reflex with state (model-based), goal-based agents, utility-based agents, and learning agents. Know advantages/disadvantages of each and when to pick one. Also discuss agent design trade-offs (sensing/modeling vs planning complexity). 


Exam tips for Unit I: be able to (a) define agent-related terms with examples, (b) classify environments and agent types, (c) state the rationality definition and give short arguments about bounded rationality and value alignment. 


---

Unit II — Problem Solving and Searching Techniques (Guidelines: Chapter 3: 3.1, 3.2; Chapter 5: 5.1; Chapter 6: 6.1, 6.2; Chapter 9: 9.1–9.4 (with exclusions) and 9.5 (with exclusions) — follow guideline precisely). 

> Note: the guideline excludes some advanced variants within Chapter 9 (DFID, Bidirectional, Branch-and-Bound, Iterative Deepening A*) — so I won’t emphasize those. 



Chapter 3 — Selected sections

3.1 Problem-solving agents

Problem formulation: state representation, initial state, actions (transition model), goal test, path cost. Define successor function. Understand formulation for classic problems (8-puzzle, path planning). Agent uses search to find action sequence. 

Search tree vs state space, solution path vs node. Recognize complexity measures: branching factor (b), depth (d), and runtime/space analysis basics. 


3.2 Example problems

Typical examples: 8-puzzle, path-finding, vacuum world, missionaries & cannibals. Know how to express them formally (states, actions, costs) and why different formulations change search complexity. 


Chapter 5 — 5.1 Defining Constraint Satisfaction Problems (CSPs)

CSP definition: variables X = {X1..Xn}, domains D_i, constraints C (relations over variables). Goal: assignment satisfying all constraints. Distinguish binary vs non-binary constraints. 

Common CSP examples: map coloring, n-queens, scheduling — understand how to model. Key idea: search + inference — use constraint propagation (e.g., arc consistency) to prune search. 

Backtracking search basic idea: depth-first assignment with constraint checking; enhancements: MRV (minimum remaining values), degree heuristic, forward checking, MAC (maintaining arc consistency). (These are later sub-sections; 5.1 is definitional — but know core terms and why CSPs can be more efficient than blind search.) 


Chapter 6 — 6.1 Game Theory and 6.2 Optimal Decisions in Games (adversarial search basics)

6.1 Game theory basics: zero-sum vs general-sum, players, utility; deterministic, perfect-information games (chess, tic-tac-toe) vs stochastic or imperfect games. Define minimax value conceptually. 

6.2 Optimal decisions: Minimax algorithm for two-player zero-sum games with perfect information. Understand recursive definition: utility of a state = max/min of children depending on player. Complexity: O(b^m). Know evaluation functions for non-terminal cutoffs. Introduce alpha-beta pruning idea (pruning irrelevant branches) — though deep alpha-beta algorithmic details are in 6.3 which guideline did not require, still good to know conceptually. 


Chapter 9 — Inference in First-Order Logic (selected sections per guideline)

Guideline includes 9.1–9.4 and 9.5 but with specific exclusions — we cover the included material:

9.1 Propositional vs First-Order inference

Difference: propositional logic has atomic sentences (no variables/functions), first-order logic (FOL) adds quantifiers, predicates, functions, variables — much more expressive. Propositional inference methods scale poorly for relational structure; FOL enables compact domain knowledge. Understand what inference tasks mean in each. 


9.2 Unification and First-Order inference

Unification: algorithm to make two terms syntactically identical by finding substitutions (most general unifier — MGU). Key for resolution, backward/forward chaining. Know the unification algorithm basics: occurs-check, variable substitution, composition. 


9.3 Forward chaining

Forward chaining (data-driven): repeatedly apply inference rules whose premises are known until goal is derived. Works well with Horn clauses and for rule-based systems; guaranteed to find solution if exists and search space finite. Note treatment of facts vs rules and efficiency tradeoffs. 


9.4 Backward chaining

Backward chaining (goal-driven): start from query, recursively reduce to subgoals using rules; used in Prolog-style systems. Efficient when only small part of KB relevant. Understand recursion, AND/OR search view of goals and subgoals. 


9.5 Resolution (note guideline excludes some specific algorithms like Branch-and-Bound & iterative deepening A*)

Resolution principle: convert FOL sentences to clausal form (CNF), then apply resolution rule (resolvent) to derive contradiction and prove entailment by refutation. Know the high-level steps: eliminate implications, move to prenex, Skolemize, CNF, then propositional-style resolution with unification. Important properties: resolution is sound and (with FOL resolution strategy) complete. 


Exam tips for Unit II: be able to formulate problems, state and contrast forward/backward chaining, describe unification step-by-step for simple terms, and explain minimax at a high level (with example). For CSPs, model one example (e.g., map coloring) and explain MRV/forward checking in a sentence.


---

Unit III — Knowledge Representation (Guideline: Chapter 4 Complete; Chapter 7 Complete except Conceptual Graphs; plus a set of cross-chapter sections listed in the guideline). 

> The guideline lists additional sections across Chapters 1–6 (1.1–1.5, 2.1–2.2, 3.1–3.2–3.4, 5.1–5.3, 6.7.2). I will include these where relevant inside the chapter summaries below. 



Chapter 4 — Search in Complex Environments (Complete)

4.1 Local search & optimization: hill climbing, simulated annealing, stochastic local search (e.g., GSAT), tabu search, gradient-based methods for continuous spaces. Know difference: local search keeps single state and improves it iteratively (vs systematic search that builds paths). Complexity: good for huge state spaces with satisficing solutions (e.g., n-queens). 

4.2 Local search in continuous spaces: gradient descent, stochastic gradient, Newton’s method intuition. Relevant when optimization is differentiable. 

4.3 Search with nondeterministic actions: AND-OR search trees to handle action outcomes; solution is a conditional plan (policy). 

4.4 Partial observability: belief states and search in belief space; planning under uncertainty requires representing knowledge distributions or sets of possible states. 

4.5 Online search and unknown environments: algorithms where agent interleaves planning & acting (e.g., LRTA* idea), exploration vs exploitation tradeoffs. 


Chapter 7 — Logical Agents (Complete except conceptual graphs)

7.1 Knowledge-based agents: agent architecture stores explicit knowledge base (KB) with inference engine. Important components: percept → update KB → query KB → decide action. 

7.2 Wumpus World: prototypical example showing reasoning under uncertainty and the need for logical inference (percepts, safe squares, etc.). Understand simple logical axioms used to infer safe moves. 

7.3 Logic basics: syntax/semantics overview; idea of entailment and inference; models and satisfiability. 

7.4 Propositional logic: sentences, connectives, truth tables, normal forms. Know how to convert small sentence to CNF and why CNF is used in resolution. 

7.5 Propositional theorem proving: truth-table method (complete but exponential), resolution for propositional logic. Understand simple example of resolution refutation. 

7.6 Effective propositional model checking: brief intro to algorithms and optimization (e.g., DPLL, SAT solvers) — know that SAT solving is practical and used widely. 

7.7 Agents based on propositional logic: how to build agents that use KB and propositional reasoning to act in simple domains. Be able to sketch a tiny KB-driven agent loop. 


Cross-referenced sections from guideline (condensed)

From Chapter 1 (1.1–1.5): foundational ideas — what AI is (we covered), foundations/history/state-of-art, risks & benefits. Good to cite when discussing motivations/ethical points in representation.

From Chapter 2 (2.1,2.2): agents & rationality — these underpin knowledge-based agent choices and performance measures. 

From Chapter 3 (3.1,3.2,3.4): problem-solving agent and example problems — tie to representation choices (how a problem encoded affects search). Also uninformed search (3.4) basics: BFS, DFS, UCS definitions and properties (completeness, optimality). 

From Chapter 5 (5.1–5.3): CSP definition (5.1 covered above), constraint propagation (5.2) and backtracking search (5.3) — core KR influence: constraints are a representation style that allows specialized inference. 

From Chapter 6 (6.7.2): limitations of game search algorithms — know practical limits (combinatorial explosion), heuristic evaluation design issues. 


Exam tips for Unit III: be ready to (a) convert simple English domain facts into propositional and first-order sentences, (b) explain how KB updates and inference work, (c) state when to use CSPs vs logic-based representation, (d) explain belief states for partial observability from Chapter 4.


---

Unit IV — Understanding Natural Languages (Guideline: Chapter 12 sections 12.1–12.4 up to recursive transition network on p.247). 

Chapter 12 — Quantifying Uncertainty (note: the guideline maps NLP to Chapter 12? — the guideline says Unit IV is Chapter 12: 12.1–12.4 up to recursive transition network on Page 247; in the book, Chapter 12 is Quantifying Uncertainty and Chapter 24 is Natural Language Processing. The guideline likely intends Chapter 24 (NLP), but it explicitly lists Chapter 12 — however the phrase “recursive transition network on Page 247” aligns with early NLP parsing material in the book (the page number matches the Parsing/grammars section). To be faithful to the guideline file as uploaded I will use the exact reference: Chapter 12 (12.1–12.4) — and also clarify this point.**

Important clarification (exam-safety): the guidelines file explicitly lists “Chapter 12: 12.1,12.2,12.3,12.4 (up to recursive transition network on Page 247)” for Unit IV. In the Russell & Norvig PDF you provided, Chapter 12 is “Quantifying Uncertainty” and Natural Language Processing is Chapter 24. There is a page-number-based reference to recursive transition networks that matches the NLP chapter—so the guideline likely intends the NLP material (parsing/RTN) but misnumbered chapter. I will cover the NLP parsing material up to recursive transition networks (RTNs) because that matches the page/phrase in the guideline and is what students expect under “Understanding Natural Languages.” If you need strictly Chapter 12 (probability) instead, tell me immediately — otherwise I’ll proceed with the NLP parsing content (as the guideline’s RTN hint suggests). I used the guideline file to detect this mapping.

(Proceeding with NLP parsing content up to recursive transition networks.)


---

Natural Language Processing — Core topics up to recursive transition networks (RTNs)

> Source: the book’s NLP chapter (table of contents and sections around parsing up to RTNs). 



Language models (brief): n-gram models and smoothing idea — approximate probability of a sentence using conditional probabilities of next word given previous n-1 words. Useful for speech recognition and simple generation. (High-level only up to parsing emphasis.) 

Grammar & formal grammars: grammars define valid sentences; Context-Free Grammars (CFGs) are central to parsing. Understand terminals/non-terminals, production rules, parse trees. 

Parsing basics: parsing maps sentence → parse tree according to grammar. Two major parsing strategies: top-down (start from start symbol expand) and bottom-up (construct parse from words up). Know concept of ambiguity: multiple parses for same sentence; disambiguation needs semantics/priority. 

Recursive Transition Networks (RTNs): RTNs are finite-state machines augmented with recursion — equivalent in power to CFGs but often more intuitive to encode grammars; used historically to represent grammars and simple parsers (RTNs look like graphs where nodes can call sub-networks). The guideline specifically stops at RTNs — know how RTNs represent nested structures and how they can be executed like a stack-based parser. 


Concrete example to study:

Given a small grammar (S → NP VP; NP → Det N | NP PP; etc.) show a short top-down parse and note ambiguity (e.g., I saw the man with a telescope — attach PP to VP or NP). Explain how RTN would encode NP and PP as sub-networks and how recursion allows nested prepositional phrases. 


Exam tips for Unit IV (NLP parsing):

Define CFG and RTN, sketch a small grammar, perform one step of top-down parsing on a short sentence, and explain ambiguity/examples. If asked about probabilities, mention n-gram models briefly. 



---

Unit V — AI: The Present and the Future (Guideline: Chapter 27 Complete and Chapter 28 Complete). 

Chapter 27 — Computer Vision (Complete) — high-yield summary

27.1 Introduction / motivation: goal is to infer 3D properties and semantics from images; discuss the pipeline view (image formation → low-level features → higher-level recognition). 

27.2 Image formation: pinhole camera model, projection geometry basics (mapping 3D points to 2D), effects of illumination and perspective — important to conceptually understand (no heavy derivations needed unless asked). 

27.3 Simple image features: edges, corners, gradients, local descriptors (SIFT/ORB mention historically) — features are building blocks for detection/recognition. 

27.4 Classifying images: formulate classification: feature extraction + classifier (SVM, CNN). Modern emphasis: deep convolutional networks are dominant for image classification; know the general CNN idea: convolution → nonlinearity → pooling → fully connected → softmax. 

27.5 Detecting objects: sliding windows / region proposals → object detectors (e.g., R-CNN family historically) and modern one-stage detectors (YOLO/SSD). Know difference between classification and detection (localization). 

27.6 The 3D world: stereo vision basics, depth estimation, structure from motion — high-level understanding of how depth can be inferred. 

27.7 Using computer vision: pipelines for real systems, performance measures, datasets (ImageNet), limitations and error modes. 


Exam tips for Unit V (Vision): define image formation, explain what a convolutional layer does, know difference between detection/classification, and give one short example of a computer vision application (medical imaging, self-driving). 

Chapter 28 — Philosophy, Ethics, and Safety of AI (Complete) — concise study guide

28.1 The limits of AI: conceptual limits (computability, complexity), engineering limits, and the challenge of general intelligence. Discuss the distinction between capability and control. 

28.2 Can machines really think?: philosophical arguments about mind and computation (Turing test, Chinese room debate, etc.) — know at least Turing test idea and criticisms. 

28.3 Ethics of AI: major themes — fairness, transparency, privacy, accountability, safety. Understand value alignment (ensuring machine objectives match human values), explainability (XAI), certification and standards (e.g., IEEE, possible regulatory frameworks). Practical topics: bias in data, adversarial attacks, privacy leakage from ML models.

Practical safety/standards: mention UL/ISO-like analogies and why ML systems need different V&V (verify training data, fairness, robustness). Also the King Midas caution: optimizing a naive objective can produce unintended harmful behavior.


Exam tips for Unit V (Ethics): give clean definitions of value alignment, explainability, and list two practical policy/engineering measures to reduce harm (data auditing and model interpretability techniques). Use examples (loan decisions, self-driving cars).


---

Quick study plan (how to use this summary in the next few hours)

Start with Unit I (2–3 short reads): Chapter 2 (agent types & rationality) + 1.1 & 1.4 to ground. 

Unit II (search & logic): focus on problem formulation (3.1), simple search examples (3.2), CSP basics (5.1), minimax idea (6.1–6.2), and first-order inference basics (9.1–9.4). Practice one small example for each (e.g., formulate 8-puzzle, trace one minimax step, do one unification example).

Unit III: read Chapter 4 (local search & partial observability) and Chapter 7 (logical agents). Do a tiny exercise: encode a few English facts into propositional/FOL/one CSP.

Unit IV: focus on grammar + parsing + RTN example. Practice drawing a parse tree. 

Unit V: read Chapter 27 (vision) overview and Chapter 28 (ethics & safety) carefully — know examples, definitions, and policy implications.



---

Sources I used (from your uploads)

The course Guidelines (explicit mapping of units →

Nice — I’ll dive deep and show how things work, step-by-step with small, exam/practice examples. I’ll stick to topics the guideline asked for and expand them: systematic (uninformed + informed) search, local/search optimization, adversarial (minimax + α-β), CSPs (with heuristics & AC-3), chaining (forward/backward), unification & resolution, and Unit IV NLP parsing up to Recursive Transition Networks — each with worked demos you can follow. This is long but focused so you can study or copy examples into paper for the exam. Ready? Let’s go.

Quick notation and reminders

b = branching factor (max children per node).

d = depth of shallowest solution.

g(n) = path cost to node n.

h(n) = heuristic estimate of remaining cost (used in A*).

In examples I'll use small graphs, tiny games, or tiny grammars so steps are tractable.



---

1. Problem formulation (short)

Before any search, define:

State representation (what information matters).

Initial state.

Actions / successor function (how states change).

Goal test (when to stop).

Path cost (optional; often number of steps or sum of action costs).


Example (tiny): Graph nodes {A,B,C,D}, edges A→B (1), A→C (2), B→D (1), C→D (1). Start=A, goal=D.


---

2. Uninformed (blind) search algorithms — inner workings + example

2.1 Breadth-First Search (BFS)

Strategy: expand shallowest unexpanded node first (FIFO queue).

Properties: complete (if finite branching), optimal when step cost = 1 for each action.

Time/Space: O(b^d) — both.


Demo (graph above, unit cost 1):

Queue initially: [A]

1. Expand A → enqueue B, C → Queue [B, C]


2. Expand B → enqueue D → Queue [C, D]


3. Expand C → enqueue D (but usually mark visited) → Queue [D]


4. Expand D → goal found.



BFS finds path A→B→D (length 2). If both paths length equal, depends on order.


---

2.2 Depth-First Search (DFS)

Strategy: expand deepest node last expanded (stack or recursion).

Properties: not guaranteed to find shortest path; can get stuck in infinite paths (unless depth limit).

Time: O(b^m) where m=maximum depth; Space: O(bm) (linear in depth).


Demo (same graph):

Stack: [A]

1. Expand A → push C then B? (order matters). If push C then B, stack top B → go deeper... If choose B first: A→B→D found quickly. If choose C first and graph had big depth, DFS could go deep.




---

2.3 Depth-Limited Search (DLS) / Iterative Deepening (ID)

DLS: DFS with max depth L. Useful to avoid infinite descent.

Iterative Deepening DFS (IDDFS): repeated DLS with increasing limits 0..d. Combines BFS optimality with DFS space advantage. Time similar to BFS for exponential trees.



---

2.4 Uniform-Cost Search (UCS)

Strategy: expand node with lowest path cost g(n) (priority queue by g).

Properties: complete (if step costs positive), optimal (returns minimal-cost path).

Time/Space exponential in general.


Demo (graph with costs: A→B (1), A→C (2), B→D (5), C→D (2)):

Priority queue by cost:

1. Expand A (g=0) → add B(g=1), C(g=2)


2. Expand B (g=1) → add D(g=6)


3. Expand C (g=2) → add D(g=4)


4. Next expand D with g=4 (cheapest D), so path A→C→D chosen (cost 4), optimal.




---

3. Informed (heuristic) search — inner workings + examples

3.1 Greedy Best-First Search

Uses heuristic h(n) and selects node with smallest h.

Not optimal. Fast but can be fooled by bad heuristic.

Think “closest according to heuristic” (no regard for path cost so far).


Demo (straight-line distance heuristic in navigation): start=A, goal=D

If heuristic estimates: h(A)=3, h(B)=2, h(C)=1, h(D)=0. Greedy expands A→C→D maybe, but could follow wrong route if h misleads.


---

3.2 A* Search — the gold standard

f(n) = g(n) + h(n). Expand node with lowest f.

Admissible heuristic: never overestimates true cost to goal → A* optimal.

Consistent (monotonic): h(n) ≤ c(n,a,n') + h(n') ensures f-values nondecreasing along path, enabling efficient implementation.


Demo (graph with costs, and heuristic = straight-line dist)

Same costed graph as UCS demo. Suppose h estimates: h(A)=3, h(B)=5, h(C)=2, h(D)=0 Start with f(A)=0+3=3 → expand A → B (g=1, f=6), C(g=2,f=4) Next expand C (lowest f=4) → add D(g=4, f=4) — solution found with cost 4 → optimal.

Step-by-step with open list (priority queue by f):

Open: [A(f=3)]

Expand A → add B(f=6), C(f=4) → Open [C(4), B(6)]

Expand C → add D(f=4), B remains → Open [D(4), B(6)]

Expand D → found goal; because f=4 matches g=4 minimal cost.



---

4. Local search & optimization (no complete path stored) — inner workings + examples

Local search keeps a single current state and modifies it iteratively.

4.1 Hill-Climbing (Greedy Local Search)

Evaluate neighbor states and move to the neighbor with better evaluation (higher utility or lower cost).

Stops at local maxima/minima (plateaus), not guaranteed global optimum.


Demo — maximize simple function f(x)=−(x−3)^2 + 10 (peak at x=3)

Start x=0, neighbors = {x±1}. Evaluate:

x=0 → neighbors −1,1 → pick best (1) if better, keep moving until x=3 reached. Works for unimodal functions.


Demo — N-Queens (constructive hill climb)

State: positions of N queens, one per column. Neighbors: move a queen within its column to other rows.

Evaluation: negative number of pairs attacking (or −attacks).

Procedure: move queen to reduce conflicts; when stuck in local minimum, use random restarts.



---

4.2 Simulated Annealing

Like hill climbing but occasionally accepts worse moves with probability exp(−ΔE/T) where T is temperature decreasing over time. Allows escape from local optima.

Temperature schedule is critical.


Demo — same N-Queens; if stuck, temperature allows jumps out of plateau.


---

4.3 Genetic Algorithms (brief)

Population of candidate solutions, selection by fitness, crossover, mutation.

Useful for poorly-behaved search spaces.



---

5. Adversarial search: Minimax and α-β pruning — inner workings + example

5.1 Minimax (perfect information, zero-sum two-player)

Players: MAX and MIN alternate moves. Each terminal state has utility U.

Minimax value V(n): if MAX to move: max over children V(child); if MIN: min over children.

Root action = child with value equal to V(root).


Tiny Demo — Tic-Tac-Toe partial tree

Simpler: minimax tree for a 2-ply game (MAX at root):

Root (MAX)
        /     \
     A         B
    / \       / \
  A1  A2    B1  B2
 U: 3   5   2   4

Compute bottom-up:

A's children A1=3, A2=5 → A (MIN to move?) careful: define players per level. If levels: root (MAX), next MIN, leaves terminal with utilities for MAX. Suppose root (MAX) chooses between A and B; at A, MIN chooses min(3,5)=3 → value(A)=3. At B, MIN chooses min(2,4)=2 → value(B)=2. Root MAX chooses max(3,2)=3 → selects A. So MAX picks the branch with value 3.


Step-by-step:

1. Evaluate terminal utilities.


2. Backpropagate minima/maxima depending on player at that node.


3. Choose child giving optimum for MAX.




---

5.2 Alpha-Beta pruning

α = best already explored option along path to MAX (lower bound).

β = best already explored option along path to MIN (upper bound).

If α ≥ β, prune remaining siblings (cannot influence outcome).


Demo (small tree):

Use the previous tree but expand order such that many branches pruned. The idea is: when evaluating one branch gives a good lower bound for MAX that is ≥ best possible for sibling under MIN, sibling can't help.

Alpha-beta gives same minimax result but with fewer node evaluations.


---

6. Constraint Satisfaction Problems (CSPs) — inner workings + worked example

6.1 CSP basics

Variables X1..Xn, each with domain Di. Constraints restrict allowed combinations.

Solve by search (assign variables) + inference (prune inconsistent values).


Example: Map coloring — regions {WA, NT, SA, Q, NSW}, colors {Red, Green, Blue}, constraints: adjacent regions != same color. (A classic).


---

6.2 Backtracking search (basic)

Assign variables one by one, backtrack on conflict.


Step demo (WA, NT, SA) with ordering WA, NT, SA:

1. WA=Red.


2. NT must != WA → can be Green or Blue. Choose Green.


3. SA must != WA and != NT (i.e., not Red, not Green) → choose Blue. Success for these three.



If SA had no legal color, backtrack to NT pick different color.


---

6.3 Heuristics to speed CSP solving

a) MRV — Minimum Remaining Values (choose variable with fewest legal values)

Reduces branching early.


b) Degree heuristic (tie-breaker) — choose variable involved in most constraints with unassigned variables.

c) Least Constraining Value — when picking a value for a variable, prefer value that rules out the fewest choices in neighbors.

d) Forward Checking

After assigning a variable, eliminate inconsistent values from neighbors’ domains. If any neighbor’s domain becomes empty → backtrack immediately.


d) Maintaining Arc Consistency (MAC) / AC-3

Arc consistency: for every pair (Xi, Xj) and for every value a in domain Xi, there exists some b in domain Xj such that constraint (Xi=a, Xj=b) satisfied. If not, remove a.

AC-3 algorithm: repeated queue of arcs; when domain of Xi changes, re-enqueue arcs (Xk, Xi) to recheck.


Demo — simple binary CSP (X, Y domains {1,2,3}; constraint X < Y)

Start domains: X:{1,2,3}, Y:{1,2,3} Check arc (X,Y): remove x-values with no y s.t. x<y → X can be 1 or 2 (3 eliminated). Now (Y,X) check: remove y-values with no x s.t. x<y → y must be 2 or 3 (1 eliminated). Continue until stable.

AC-3 prunes domains before search → massive speedups.


---

6.4 Example: 4-Queens CSP (classic local search and CSP encoding)

Variables: Q1..Q4 representing row of queen in each column (domain {1..4}).

Constraints: no two queens attack → rows differ and |col_i − col_j| ≠ |row_i − row_j|.

Use local search (hill climbing with restarts) or CSP backtracking with MRV/forward checking.



---

7. Logical inference: forward chaining, backward chaining, unification, resolution — with examples

We’ll use a small knowledge base about family relations.

Knowledge (Horn rules & facts)

1. Parent(alice, bob).  (alice is parent of bob)


2. Parent(bob, carol).


3. Parent(alice, david).


4. Ancestor(x, y) :- Parent(x, y). 5


